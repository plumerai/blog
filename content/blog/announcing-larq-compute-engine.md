---
title: "Announcing Larq Compute Engine v0.1"
subtitle: "Optimized BNN inference for edge devices"
heroImage: "/images/lce-announcement-hero.png"
date: 2020-02-17T13:00:00+01:00
draft: false
socialTitle: "Announcing Larq Compute Engine v0.1"
socialDescription: "Larq Compute Engine is a highly optimized inference engine for binarized neural networks."
socialImage: "images/lce-announcement-hero.png"
---

We believe BNNs are the future of efficient inference, which is why we've developed tools to make it easier to train and research these models.
Our open-source library [Larq](https://larq.dev) enables developers to build and train BNNs and integrates seamlessly with TensorFlow Keras.
[Larq Zoo](https://docs.larq.dev/zoo) provides implementations of major BNNs from the literature together with pretrained weights for state-of-the-art models.

But the ultimate goal of BNNs is to solve real-world problems on the edge.
So once you've built and trained a BNN with Larq, how do you get it ready for efficient inference?
Today, we're introducing Larq Compute Engine to tackle that problem.

[Continue reading on the Larq blog...](https://blog.larq.dev/2020/02/announcing-larq-compute-engine/)